from dotenv import load_dotenv

load_dotenv()

import ollama
from langsmith import traceable

MAX_ITERATIONS = 10
MODEL = "llama3.2"

# ----- Tools ----- 

@traceable(run_type="tool")
def get_product_price(product: str) -> float:
    """Look up the price of a product in the catalog."""
    print(f" >> Executing get_product_price(product= '{product}')")
    prices = {"laptop": 1299.99, "headphones": 89.00, "keyboard": "79.65"}
    return prices.get(product, 0)

@traceable(run_type="tool")
def apply_discount(price: float, discount_tier: str) -> float:
    """Apply a discount tier to a price and return the final price.
    Available tiers: bronze, silver, gold."""
    print(f" >> Executing apply_discount on product of price={price} and discount_tier={discount_tier}")
    discount_percentages = {"bronze": 5, "silver": 12, "gold": 23}
    discount = discount_percentages.get(discount_tier, 0)
    return round(price * (1-discount/100), 2)

# difference: without @tool, we must manually define the json schema for each function.
# this is what exactly langchain @tool decorator generates automatically
# auto generated by langchain

tools_for_llm = [
    {
        "type": "function",
        "function": {
            "name": "get_product_price",
            "description": "Look up the price of product in catalog.",
            "parameters": {
                "type": "object",
                "properties": {
                    "product": {
                        "type": "string",
                        "description": "The product name e.g. 'laptop'"
                    }
                },
                "required": ["product"],
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "apply_discount",
            "description": "Apply a discount tier to a price and return the final price. Avialable tiers: bronze, silver, gold.",
            "parameters": {
                "type": "object",
                "properties": {
                    "price": {
                        "type": "number",
                        "description": "The original price"
                    },
                    "discount_tier": {
                        "type": "string",
                        "description": "The discount tier: bronze, silver, gold"
                    }
                },
                "required": ["price", "discount_tier"],
            }
        }
    }
]

@traceable(name="Ollama Chat", run_type="llm")
def ollama_chat_traced(messages):
    return ollama.chat(model=MODEL, tools=tools_for_llm, messages=messages)

# -- Agent loop --

@traceable(name="Langchain agent loop")
def run_agent(question: str):
    tools_dict = {
        "get_product_price": get_product_price,
        "apply_discount": apply_discount
    }

    message = [
        {"role": "system", 
         "content": (
             "You are a helpful shopping assistant. "
                "You have access to product catalog tool "
                "and a discount tool.\n\n"
                "WORKFLOW - Follow this exact sequence:\n"
                "Step 1: If user asks for a product price, call get_product_price with the product name as a string\n"
                "Step 2: If user mentions ANY discount (bronze/silver/gold), you MUST call apply_discount with the price from Step 1\n"
                "Step 3: Return the final discounted price to the user\n\n"
                "EXAMPLE:\n"
                "User: 'What is the price of laptop after applying gold discount?'\n"
                "You must: 1) Call get_product_price('laptop') 2) Call apply_discount(price_from_step1, 'gold') 3) Return final price\n\n"
                "STRICT RULES:\n"
                "- NEVER skip the apply_discount tool when discount is mentioned\n"
                "- NEVER calculate discounts manually\n"
                "- Pass product names as simple strings, not dictionaries\n"
                "- Available discount tiers: bronze, silver, gold"
         ),
        },
        {
            "role": "user", 
            "content": question
        },
    ]

    for iteration in range(1, MAX_ITERATIONS+1):
        print(f"\n--- Iteration {iteration} ---")
       
        response = ollama_chat_traced(messages=message)
        ai_message = response.message

        tool_calls = ai_message.tool_calls

        #if no tool call, this is the final answer
        if not tool_calls:
            print(f"\nFinal Answer: {ai_message.content}")
            return ai_message.content
        
        #Process only the first tool call
        tool_call = tool_calls[0]
        tool_name = tool_call.function.name
        tool_args = tool_call.function.arguments

        print(f"[Tool selected]: {tool_name} with args: {tool_args}")

        tool_to_use = tools_dict.get(tool_name)
        if tool_to_use is None:
            raise ValueError(f"Tool '{tool_name}' not found")
        
        try:
            observation = tool_to_use(**tool_args)
        except Exception as e:
            print(f" [Tool Error]: {e}")
            # Send error back to the LLM so it can try again with correct parameters
            observation = f"Error: {str(e)}. Please check your parameters and try again."

        print(f" [Tool Result]: {observation}")

        message.append(ai_message) #feeding llm with the result again
        message.append(
            {
                "role": "tool",
                "content": str(observation)
            }
        )   #feeding the tool message as well back to llm

    print("Error: Max iterations reached without final answer!")
    return None



if __name__ == "__main__":
    print("Hello Langchain Agent")
    print()
    result = run_agent("What is the price of laptop after applying gold discount?")
